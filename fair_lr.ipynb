{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"seldonian_classification.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ApDLEeKfw9sJ"},"source":["## Prep"]},{"cell_type":"code","metadata":{"id":"lq9fua9wgzs4"},"source":["!pip install -U kaleido plotly"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ykAm3mTzhkSJ"},"source":["!pip install ray "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LjpEsz0scq5A"},"source":["!pip install fairlearn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m0zBI-5HECe2"},"source":["!pip install cma"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jSBAKaSo9WX7"},"source":["!pip install dalex -U"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ilaQN91F1dU"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlOztF-2hrQ8"},"source":["## Helpers"]},{"cell_type":"code","metadata":{"id":"asQheoUfw_Mr"},"source":["import math\n","import numpy as np\n","import pandas as pd\n","import sys\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, f1_score\n","from scipy.stats import t\n","from scipy.optimize import minimize # The black-box optimization algorithm used to find a candidate solution\n","from numba import jit       # Just-in-Time (JIT) compiler to accelerate Python code\n","# import ray \n","import timeit               # To time the execution of ours experiments\n","import cma\n","from time import time\n","import warnings\n","\n","np.set_printoptions(precision=5, suppress=True)\n","\n","\n","# This function returns the inverse of Student's t CDF using the degrees of freedom in nu for the corresponding\n","# probabilities in p. It is a Python implementation of Matlab's tinv function: https://www.mathworks.com/help/stats/tinv.html\n","def tinv(p, nu):\n","    return t.ppf(p, nu)\n","\n","\n","# This function computes the sample standard deviation of the vector v, with Bessel's correction\n","def stddev(v):\n","    n = v.size\n","    variance = (np.var(v) * n) / (n-1) # Variance with Bessel's correction\n","    return np.sqrt(variance)           # Compute the standard deviation\n","\n","\n","# This function computes a (1-delta)-confidence upper bound on the expected value of a random\n","# variable using Student's t-test. It analyzes the data in v, which holds i.i.d. samples of the random variable.\n","# The upper confidence bound is given by \n","#    sampleMean + sampleStandardDeviation/sqrt(n) * tinv(1-delta, n-1),\n","#    where n is the number of points in v.\n","def ttestUpperBound(v, delta):\t\n","    n  = v.size\n","    res = v.mean() + stddev(v) / math.sqrt(n) * tinv(1.0 - delta, n - 1)\n","    return res\n","\n","\n","# This function works similarly to ttestUpperBound, but returns a conservative upper bound. It uses \n","# data in the vector v (i.i.d. samples of a random variable) to compute the relevant statistics \n","# (mean and standard deviation) but assumes that the number of points being analyzed is k instead of |v|.\n","# This function is used to estimate what the output of ttestUpperBound would be if it were to\n","# be run on a new vector, v, containing values sampled from the same distribution as\n","# the points in v. The 2.0 factor in the calculation is used to double the width of the confidence interval,\n","# when predicting the outcome of the safety test, in order to make the algorithm less confident/more conservative.\n","def predictTTestUpperBound(v, delta, k):\n","    # conservative prediction of what the upper bound will be in the safety test for the a given constraint\n","    res = v.mean() + 2.0 * stddev(v) / math.sqrt(k) * tinv(1.0 - delta, k - 1)\n","    return res\n","\n","\n","def calc_fp_sklearn(X, Y, clf):\n","  y_pred = clf.predict(X)\n","  tn, fp, fn, tp = confusion_matrix(Y, y_pred).ravel()\n","  return fp / (fp + tn)\n","  \n","def calc_odds_sklearn(X, Y, clf):\n","\tx_male = X[X[:, 0] == 1]\n","\ty_pred_male = clf.predict(x_male)\n","\ty_male = Y[X[:, 0] == 1]\n","\n","\tx_female = X[X[:, 0] == 0]\n","\ty_pred_female = clf.predict(x_female)\n","\ty_female = Y[X[:, 0] == 0]\n","\n","\ttnr_male, fpr_male, fnr_male, tpr_male = confusion_matrix(y_male, y_pred_male, normalize=\"true\").ravel()\n","\ttnr_female, fpr_female, fnr_female, tpr_female = confusion_matrix(y_female, y_pred_female, normalize=\"true\").ravel()\n","\n","\treturn max(abs(fpr_female - fpr_male), abs(tpr_female - tpr_male))\n"," \n","def calc_eq_op_sklearn(X, Y, clf):\n","\tx_male = X[X[:, 0] == 1]\n","\ty_pred_male = clf.predict(x_male)\n","\ty_male = Y[X[:, 0] == 1]\n","\n","\tx_female = X[X[:, 0] == 0]\n","\ty_pred_female = clf.predict(x_female)\n","\ty_female = Y[X[:, 0] == 0]\n","\n","\ttnr_male, fpr_male, fnr_male, tpr_male = confusion_matrix(y_male, y_pred_male, normalize=\"true\").ravel()\n","\ttnr_female, fpr_female, fnr_female, tpr_female = confusion_matrix(y_female, y_pred_female, normalize=\"true\").ravel()\n","\n","\treturn abs(fnr_female - fnr_male)\n","\n","def calc_demographic_parity_sklearn(X, Y, clf):\n","  y_pred = clf.predict(X)\n","  return y_pred.sum() / X.shape[0]\n","\n","\n","def cmaes_minimize(evalf, n_features, sigma0=0.0001, restarts=20, n_iters=1000, theta0=None):\n","\t\thas_theta0 = not(theta0 is None)\n","\t\tnp.random.seed(np.floor(100000*(time()%10)).astype(int))\n","\t\ttheta_opt = None\n","\t\tval_min = np.inf\n","\n","\t\ttheta0 = theta0 if not(theta0 is None) else np.zeros(n_features)\n","\t\tnext_theta0 = theta0.copy()\n","\t\tfor _ in range(restarts):\n","\t\t\toptions = {'verb_log':0, 'verbose':-9, 'verb_disp':0, 'tolfun':1e-12, 'seed':make_seed(), 'maxiter':n_iters}\n","\t\t\tes = cma.CMAEvolutionStrategy(next_theta0, sigma0, options)\n","\t\t\twith warnings.catch_warnings():\t\t\t\t\n","\t\t\t\twarnings.simplefilter('ignore', category=RuntimeWarning)\n","\t\t\t\tes.optimize(evalf)\n","\t\t\ttheta = es.result.xbest\n","\t\t\tif not(theta is None):\n","\t\t\t\tvalue = evalf(theta)\n","\t\t\t\tif value < val_min:\n","\t\t\t\t\ttheta_opt = theta.copy()\n","\t\t\t\t\tval_min = value\n","\t\t\tnext_theta0 = theta0 + np.random.normal(size=theta0.shape)\n","\t\treturn { 'x': theta_opt }\n","\n","\n","def make_seed(digits=8, random_state=np.random):\n","    return np.floor(random_state.rand()*10**digits).astype(int)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-ol2W5fbhuK7"},"source":["## Seldonian Algorithm"]},{"cell_type":"code","metadata":{"id":"yiDoFh_ixAYX"},"source":["# Generate numPoints data points\n","def generateData(numPoints):\n","    X = pd.DataFrame([\n","                np.random.binomial(size=numPoints, n = 1, p = 0.3),\n","                np.random.normal(0.0, 1.0, numPoints),\n","            ]).T.values # Sample x from a standard normal distribution\n","\n","    intercept = (np.random.rand(1))[0]\n","    z = intercept\n","    coefs = np.random.rand(X.shape[1]) * 10\n","    z = z + np.dot(coefs, X.T)\n","    Y = np.round(1/(1+np.exp(-z))) # Set y to be x, plus noise from a standard normal distribution\n","    print('Original params: ', intercept, coefs)\n","    return (X,Y)\n","\n","\n","# Uses the weights in theta to predict the output value, y, associated with the provided x.\n","# This function assumes we are performing linear regression, so that theta has\n","# two elements: the y-intercept (first parameter) and slope (second parameter)\n","def predict(theta, x):\n","    return 1/(1 + np.exp(-np.dot(theta, x)))\n","\n","def logistic_cost(y_i, y_hat):\n","    return -y_i*np.log(y_hat) - (1-y_i)*np.log(1-y_hat)\n","\n","# Estimator of the primary objective, in this case, the negative sample mean squared error\n","def fHat(theta, X, Y):\n","    n = X.shape[0]       # Number of points in the data set\n","    res = 0.0           # Used to store the sample MSE we are computing\n","    for i in range(n):  # For each point X[i] in the data set ...\n","        prediction = predict(theta, [1, *X[i, :]])  # Get the prediction using theta\n","        prediction = np.clip(prediction, 1e-5, 1 - 1e-5)\n","        cost = logistic_cost(Y[i], prediction) # Add the squared error to the result\n","        res += cost\n","\n","    res /= n            # Divide by the number of points to obtain the sample mean squared error\n","    return -res         # Returns the negative sample mean squared error\n","\n","\n","# Returns unbiased estimates of g_1(theta), computed using the provided data\n","def g_max_loss(maximum = 0.1):\n","  def _(theta, X, Y):\n","    n = X.shape[0]       # Number of points in the data set\n","    res = np.zeros(n)   # We will get one estimate per point; initialize res to store these estimates\n","    for i in range(n):  # For each point X[i] in the data set ...\n","        prediction = predict(theta, [1, *X[i, :]])  # Get the prediction using theta\n","        prediction = np.clip(prediction, 1e-5, 1 - 1e-5)\n","        res[i] = logistic_cost(Y[i], prediction)  # Compute the squared error for the i-th data point'\n","    res = res - maximum     # We want the MSE to be less than 2.0, so g(theta) = MSE-2.0\n","    return pd.Series(res).dropna().values\n","  return _\n","\n","\n","# Returns unbiased estimates of g_2(theta), computed using the provided data\n","def g_min_loss(minimum = 1e-12):\n","  def _(theta, X, Y):\n","    n = X.shape[0]      # Number of points in the data set\n","    res = np.zeros(n)   # We will get one estimate per point; initialize res to store these estimates\n","    for i in range(n):  # For each point X[i] in the data set ...\n","        prediction = predict(theta, [1, *X[i, :]])  # Get the prediction using theta\n","        res[i] = logistic_cost(Y[i], prediction)  # Compute the squared error for the i-th data point\n","    res = minimum - res    # We want the MSE to be at least 1.25, so g(theta) = 1.25-MSE\n","    return pd.Series(res).dropna().values\n","  return _\n","\n","\n","def calc_f1(X, Y, predict_prob):\n","  y_pred = []\n","  for (data, y) in zip(X, Y):\n","    prediction = predict_prob([1, *data])  # Get the prediction using theta\n","    prediction_rounded = round(prediction)\n","    y_pred.append(prediction_rounded)\n","\n","  return f1_score(Y, y_pred, average='macro')\n","\n","def g_min_f1(minimum = 0.5, batch_size = 100):\n","  def _(theta, X, Y):\n","    n = X.shape[0]      # Number of points in the data set\n","    num_batches = int(np.floor(n / batch_size))\n","    res = np.array([])\n","    indices = np.array(range(n))\n","    \n","    def predict_prob(x):\n","      return predict(theta, x)\n","\n","    for batch in get_batch(indices, num_batches):\n","        res = np.append(res, calc_f1(X[batch, :], Y[batch], predict_prob))\n","    res = minimum - res\n","    return pd.Series(res).dropna().values\n","  return _\n","\n","\n","def calc_fp_diff(X, Y, predict_prob):\n","    male_X = X[X[:, 0] == 1,:]\n","    female_X = X[X[:, 0] == 0,:]\n","    male_Y = Y[X[:, 0] == 1]\n","    female_Y = Y[X[:, 0] == 0]\n","    \n","    male_pred_Y = predict_prob(np.hstack((np.ones(male_X.shape[0]).reshape((male_X.shape[0],1)), male_X)))\n","    male_pred_Y = np.round(male_pred_Y)\n","\n","    male_tn, male_fp, male_fn, male_tp = confusion_matrix(male_Y, male_pred_Y.T, labels=[0,1]).ravel()\n","\n","    female_pred_Y = predict_prob(\n","        np.hstack(\n","          (\n","            np.ones(female_X.shape[0]).reshape((female_X.shape[0],1)), \n","            female_X\n","          )\n","        )\n","    )\n","    female_pred_Y = np.round(female_pred_Y)\n","    female_tn, female_fp, female_fn, female_tp = confusion_matrix(female_Y, female_pred_Y.T, labels=[0,1]).ravel()\n","\n","    # https://stackoverflow.com/a/31324768/6008808\n","    fp_male = male_fp / (male_fp + male_tn) if male_fp + male_tn > 0 else 0\n","    fp_female = female_fp / (female_fp + female_tn) if female_fp + female_tn > 0 else 0\n","\n","    return np.abs(fp_female - fp_male)\n","\n","# https://stackoverflow.com/a/8290508/6008808\n","def get_batch(iterable, n=1):\n","    l = len(iterable)\n","    for ndx in range(0, l, n):\n","        yield iterable[ndx:min(ndx + n, l)]\n","\n","\n","def g_fp_diff(\n","    maximum = 0.05,\n","    batch_size = 10\n","):\n","  def _(theta, X, Y):\n","    n = X.shape[0]      # Number of points in the data set\n","    res = np.array([])\n","    \n","    def predict_prob(x):\n","      return predict(x, theta)\n","\n","    batches = []\n","    while len(batches) < n:\n","      train_x, _, train_y, _ = train_test_split(X, Y, train_size=batch_size, random_state=len(batches))\n","      batches.append((train_x, train_y))\n","\n","    for x, y in batches:\n","      res = np.append(res, calc_fp_diff(x, y, predict_prob))\n","\n","    res = res - maximum\n","    return pd.Series(res).dropna().values\n","  return _\n","\n","def calc_eq_odds_diff(X, Y, predict_prob):\n","  male_X = X[X[:, 0] == 1,:]\n","  female_X = X[X[:, 0] == 0,:]\n","  male_Y = Y[X[:, 0] == 1]\n","  female_Y = Y[X[:, 0] == 0]\n","\n","  male_pred_Y = predict_prob(np.hstack((np.ones(male_X.shape[0]).reshape((male_X.shape[0],1)), male_X)))\n","  male_pred_Y = np.round(male_pred_Y)\n","\n","  tn_male, fp_male, fn_male, tp_male = confusion_matrix(male_Y, male_pred_Y.T, labels=[0,1]).ravel()\n","\n","  female_pred_Y = predict_prob(np.hstack((np.ones(female_X.shape[0]).reshape((female_X.shape[0],1)), female_X)))\n","  female_pred_Y = np.round(female_pred_Y)\n","  tn_female, fp_female, fn_female, tp_female = confusion_matrix(female_Y, female_pred_Y.T, labels=[0,1]).ravel()\n","\n","  fpr_male = fp_male / (fp_male + tn_male) if (fp_male + tn_male) > 0 else 0\n","  fpr_female = fp_female / (fp_female + tn_female) if (fp_female + tn_female) > 0 else 0\n","\n","  tpr_male = tp_male / (tp_male + fn_male) if (tp_male + fn_male) > 0 else 0\n","  tpr_female = tp_female / (tp_female + fn_female) if (tp_female + fn_female) > 0 else 0\n","\n","  return max(abs(fpr_female - fpr_male), abs(tpr_female - tpr_male))\n","\n","\n","def g_eq_odds_diff(\n","    maximum = 0.1,\n","    batch_size = 100,\n","    bootstrap_num = 500\n","):\n","  def _(theta, X, Y):\n","    # n = X.shape[0]\n","    res = np.array([])\n","    \n","    def predict_prob(x):\n","      return predict(x, theta)\n","\n","    batches = []\n","    while len(batches) < bootstrap_num:\n","      train_x, _, train_y, _ = train_test_split(X, Y, train_size=batch_size, random_state=len(batches))\n","      batches.append((train_x, train_y))\n","\n","    for x, y in batches:\n","      res = np.append(res, calc_eq_odds_diff(x, y, predict_prob))\n","\n","    res = res - maximum\n","    return pd.Series(res).dropna().values\n","  return _\n","\n","\n","def calc_eq_op_diff(X, Y, predict_prob):\n","  male_X = X[X[:, 0] == 1,:]\n","  female_X = X[X[:, 0] == 0,:]\n","  male_Y = Y[X[:, 0] == 1]\n","  female_Y = Y[X[:, 0] == 0]\n","\n","  male_pred_Y = predict_prob(np.hstack((np.ones(male_X.shape[0]).reshape((male_X.shape[0],1)), male_X)))\n","  male_pred_Y = np.round(male_pred_Y)\n","\n","  tnr_male, fpr_male, fnr_male, tpr_male = confusion_matrix(male_Y, male_pred_Y.T, labels=[0,1], normalize='true').ravel()\n","\n","  female_pred_Y = predict_prob(np.hstack((np.ones(female_X.shape[0]).reshape((female_X.shape[0],1)), female_X)))\n","  female_pred_Y = np.round(female_pred_Y)\n","  tnr_female, fpr_female, fnr_female, tpr_female = confusion_matrix(female_Y, female_pred_Y.T, labels=[0,1], normalize='true').ravel()\n","\n","  return abs(fnr_female - fnr_male)\n","\n","\n","def g_eq_op_diff(\n","    maximum = 0.1,\n","    batch_size = 100,\n","    bootstrap_num = 500\n","):\n","  def _(theta, X, Y):\n","    # n = X.shape[0]\n","    res = np.array([])\n","    \n","    def predict_prob(x):\n","      return predict(x, theta)\n","\n","    batches = []\n","    while len(batches) < bootstrap_num:\n","      train_x, _, train_y, _ = train_test_split(X, Y, train_size=batch_size, random_state=len(batches))\n","      batches.append((train_x, train_y))\n","\n","    for x, y in batches:\n","      res = np.append(res, calc_eq_op_diff(x, y, predict_prob))\n","\n","    res = res - maximum\n","    return pd.Series(res).dropna().values\n","  return _\n","\n","\n","def get_logistic_regression_initial_params(X, Y):\n","    reg = LogisticRegression(max_iter=10000).fit(X, Y)\n","    theta0 = reg.intercept_[0]   # Gets theta0, the y-intercept coefficient\n","    thetas = np.array([theta0, *reg.coef_[0]])\n","    print(\"initial params\", thetas)\n","    return thetas\n","\n","# Our Quasi-Seldonian linear regression algorithm operating over data (X,Y).\n","# The pair of objects returned by QSA is the solution (first element) \n","# and a Boolean flag indicating whether a solution was found (second element).\n","def QSA(X, Y, gHats, deltas):\n","    # Put 75% of the data in candidateData (D1), and the rest in safetyData (D2)\n","    candidateData_len = 0.75\n","    candidateData_X, safetyData_X, candidateData_Y, safetyData_Y = train_test_split(\n","                                X, Y, test_size=1-candidateData_len, shuffle=False)\n","\n","    # Get the candidate solution\n","    candidateSolution = getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, safetyData_X.size)\n","\n","    # Run the safety test\n","    passedSafety      = safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas)\n","\n","    # Return the result and success flag\n","    return [candidateSolution, passedSafety]\n","\n","\n","# Run the safety test on a candidate solution. Returns true if the test is passed.\n","#   candidateSolution: the solution to test. \n","#   (safetyData_X, safetyData_Y): data set D2 to be used in the safety test.\n","#   (gHats, deltas): vectors containing the behavioral constraints and confidence levels.\n","def safetyTest(candidateSolution, safetyData_X, safetyData_Y, gHats, deltas):\n","\n","    for i in range(len(gHats)):\t# Loop over behavioral constraints, checking each\n","        g         = gHats[i]\t# The current behavioral constraint being checked\n","        delta     = deltas[i]\t# The confidence level of the constraint\n","\n","        # This is a vector of unbiased estimates of g(candidateSolution)\n","        g_samples = g(candidateSolution, safetyData_X, safetyData_Y) \n","\n","        # Check if the i-th behavioral constraint is satisfied\n","        upperBound = ttestUpperBound(g_samples, delta) \n","\n","        print('safety', upperBound)\n","\n","        if upperBound > 0.0: # If the current constraint was not satisfied, the safety test failed\n","            return False\n","\n","    # If we get here, all of the behavioral constraints were satisfied\t\t\t\n","    return True\n","\n","\n","# The objective function maximized by getCandidateSolution.\n","#     thetaToEvaluate: the candidate solution to evaluate.\n","#     (candidateData_X, candidateData_Y): the data set D1 used to evaluated the solution.\n","#     (gHats, deltas): vectors containing the behavioral constraints and confidence levels.\n","#     safetyDataSize: |D2|, used when computing the conservative upper bound on each behavioral constraint.\n","def candidateObjective(thetaToEvaluate, candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize):\t\n","\n","    # Get the primary objective of the solution, fHat(thetaToEvaluate)\n","    result = fHat(thetaToEvaluate, candidateData_X, candidateData_Y)\n","    predictSafetyTest = True     # Prediction of what the safety test will return. Initialized to \"True\" = pass\n","    for i in range(len(gHats)):  # Loop over behavioral constraints, checking each\n","        g         = gHats[i]       # The current behavioral constraint being checked\n","        delta     = deltas[i]      # The confidence level of the constraint\n","\n","        # This is a vector of unbiased estimates of g_i(thetaToEvaluate)\n","        g_samples = g(thetaToEvaluate, candidateData_X, candidateData_Y)\n","\n","        # Get the conservative prediction of what the upper bound on g_i(thetaToEvaluate) will be in the safety test\n","        upperBound = predictTTestUpperBound(g_samples, delta, safetyDataSize)\n","\n","        print('candidate', upperBound)\n","        \n","        # We don't think the i-th constraint will pass the safety test if we return this candidate solution\n","        if upperBound > 0.0:\n","\n","            if predictSafetyTest:\n","                # Set this flag to indicate that we don't think the safety test will pass\n","                predictSafetyTest = False  \n","\n","                # Put a barrier in the objective. Any solution that we think will fail the safety test will have a\n","                # large negative performance associated with it\n","                result = -100000.0    \n","\n","            # Add a shaping to the objective function that will push the search toward solutions that will pass \n","            # the prediction of the safety test\n","            result = result - upperBound\n","    \n","    # Negative because our optimizer (Powell) is a minimizer, but we want to maximize the candidate objective\n","    return -result  \n","\n","\n","# Use the provided data to get a candidate solution expected to pass the safety test.\n","#    (candidateData_X, candidateData_Y): data used to compute a candidate solution.\n","#    (gHats, deltas): vectors containing the behavioral constraints and confidence levels.\n","#    safetyDataSize: |D2|, used when computing the conservative upper bound on each behavioral constraint.\n","def getCandidateSolution(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize):\n","\n","    # Chooses the black-box optimizer we will use (Powell)\n","    minimizer_method = 'Powell'\n","    minimizer_options={'disp': True, 'maxiter': 1000}\n","\n","    # Initial solution given to Powell: simple linear fit we'd get from ordinary least squares linear regression\n","    initialSolution  = get_logistic_regression_initial_params(candidateData_X, candidateData_Y)\n","\n","    # Use Powell to get a candidate solution that tries to maximize candidateObjective\n","    res = minimize(candidateObjective, x0=initialSolution, method=minimizer_method, options=minimizer_options,\n","        args=(candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize))\n","    \n","    def eval_func(theta):\n","      return candidateObjective(theta, candidateData_X, candidateData_Y, gHats, deltas, safetyDataSize)\n","\n","    # res = cmaes_minimize(evalf=eval_func, n_features=candidateData_X.shape[1] + 1, theta0=initialSolution)\n","\n","    # Return the candidate solution we believe will pass the safety test\n","    return res.x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tj5MAcyShzi-"},"source":["## Playground"]},{"cell_type":"code","metadata":{"id":"KNNqbaE6FN1r"},"source":["X,Y = generateData(10000)\n","n = X.shape[0]\n","res = 0.0\n","for i in range(n):  # For each point X[i] in the data set ...\n","  prediction = predict(np.array([1,2,3,4,5]), [1, *X[i, :]])  # Get the prediction using theta\n","  res += logistic_cost(Y[i], prediction) # Add the squared error to the result\n","res /= n            # Divide by the number of points to obtain the sample mean squared error\n","print(res)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0CE2qT_77xvZ"},"source":["calc_fp_diff([12, 1.14, 7.52], X, Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXLvpd-x7xvc"},"source":["np.mean(gHat3([12, 1.14, 7.52], X, Y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WPlfj49BcsBn"},"source":["X[[1,2,3],:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J8PpAnWZR3lj"},"source":["gHat3([1,2,3], X, Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fE0HYtGxc5ai"},"source":["X = pd.DataFrame([\n","            np.random.binomial(size=numPoints, n=1, p= 0.5),\n","            np.random.normal(0.0, 1.0, numPoints), \n","        ]).T.values # Sample x from a standard normal distribution\n","\n","z = (np.random.rand(1))[0]\n","coefs = np.random.rand(X.shape[1]) * 100\n","print(coefs.shape)\n","print(X.shape)\n","z = z + np.dot(np.random.rand(X.shape[1]) * 100,  X.T)\n","Y = np.round(1/(1+np.exp(-z))) # Set y to be x, plus noise from a standard normal distribution"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iPaLOKCWdz5L"},"source":["2/3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ANVX3xnxh84W"},"source":["## Experiment setup"]},{"cell_type":"code","metadata":{"id":"DL27-Vw3hZfL"},"source":["@ray.remote\n","def run_experiments(worker_id, nWorkers, ms, numM, numTrials, mTest):\n","\n","    # Results of the Seldonian algorithm runs\n","    seldonian_solutions_found = np.zeros((numTrials, numM)) # Stores whether a solution was found (1=True,0=False)\n","    seldonian_failures_g1     = np.zeros((numTrials, numM)) # Stores whether solution was unsafe, (1=True,0=False), for the 1st constraint, g_1\n","    seldonian_failures_g2     = np.zeros((numTrials, numM)) # Stores whether solution was unsafe, (1=True,0=False), for the 2nd constraint, g_2\n","    seldonian_failures_g3     = np.zeros((numTrials, numM)) # Stores whether solution was unsafe, (1=True,0=False), for the 2nd constraint, g_2\n","    seldonian_fs              = np.zeros((numTrials, numM)) # Stores the primary objective values (fHat) if a solution was found\n","\n","    # Results of the Least-Squares (LS) linear regression runs\n","    LS_solutions_found = np.ones((numTrials, numM))  # Stores whether a solution was found. These will all be true (=1)\n","    LS_failures_g1     = np.zeros((numTrials, numM)) # Stores whether solution was unsafe, (1=True,0=False), for the 1st constraint, g_1\n","    LS_failures_g2     = np.zeros((numTrials, numM)) # Stores whether solution was unsafe, (1=True,0=False), for the 2nd constraint, g_2\n","    LS_failures_g3     = np.zeros((numTrials, numM)) # Stores whether solution was unsafe, (1=True,0=False), for the 2nd constraint, g_2\n","    LS_fs              = np.zeros((numTrials, numM)) # Stores the primary objective values (f) if a solution was found\n","\n","    # Prepares file where experiment results will be saved\n","    experiment_number = worker_id\n","    bin_path = './experiment_results/bin/'\n","    outputFile = bin_path + 'results%d.npz' % experiment_number\n","    print(\"Writing output to\", outputFile)\n","\n","    # Generate the data used to evaluate the primary objective and failure rates\n","    np.random.seed( (experiment_number+1) * 9999 )\n","    (testX, testY) = generateData(mTest) \n","\n","    for trial in range(numTrials):\n","        for (mIndex, m) in enumerate(ms):\n","\n","            # Generate the training data, D\n","            base_seed         = (experiment_number * numTrials)+1\n","            np.random.seed(base_seed+trial) # done to obtain common random numbers for all values of m\t\t\t\n","            (trainX, trainY)  = generateData(m)\n","\n","            # Run the Quasi-Seldonian algorithm\n","            (result, passedSafetyTest) = QSA(trainX, trainY, gHats, deltas)\n","            if passedSafetyTest:\n","                seldonian_solutions_found[trial, mIndex] = 1\n","                trueLoss = -fHat(result, testX, testY)                               # Get the \"true\" mean squared error using the testData\n","                seldonian_failures_g1[trial, mIndex] = 1 if trueLoss > 0.05  else 0   # Check if the first behavioral constraint was violated\n","                seldonian_failures_g2[trial, mIndex] = 1 if trueLoss < 1 else 0 # Check if the second behavioral constraint was violated\n","                seldonian_failures_g3[trial, mIndex] = 1 if calc_fp_diff(result, testX, testY) < 0.08 else 0\n","                \n","                seldonian_fs[trial, mIndex] = -trueLoss                              # Store the \"true\" negative mean-squared error\n","                print(f\"[(worker {worker_id}/{nWorkers}) Seldonian trial {trial+1}/{numTrials}, m {m}] A solution was found: [{result[0]:.10f}, {result[1]:.10f}]\\tfHat over test data: {trueMSE:.10f}\")\n","            else:\n","                seldonian_solutions_found[trial, mIndex] = 0             # A solution was not found\n","                seldonian_failures_g1[trial, mIndex]     = 0             # Returning NSF means the first constraint was not violated\n","                seldonian_failures_g2[trial, mIndex]     = 0             # Returning NSF means the second constraint was not violated\n","                seldonian_failures_g3[trial, mIndex]     = 0\n","                seldonian_fs[trial, mIndex]              = None          # This value should not be used later. We use None and later remove the None values\n","                print(f\"[(worker {worker_id}/{nWorkers}) Seldonian trial {trial+1}/{numTrials}, m {m}] No solution found\")\n","\n","            # Run the Least Squares algorithm\n","            theta = get_logistic_regression_initial_params(trainX, trainY)                              # Run least squares linear regression\n","            trueLoss = -fHat(theta, testX, testY)                         # Get the \"true\" mean squared error using the testData\n","            LS_failures_g1[trial, mIndex] = 1 if trueLoss > 0.05  else 0   # Check if the first behavioral constraint was violated\n","            LS_failures_g2[trial, mIndex] = 1 if trueLoss < 1 else 0   # Check if the second behavioral constraint was violated\n","            LS_failures_g3[trial, mIndex] = 1 if calc_fp_diff(theta, testX, testY) < 0.08 else 0\n","            \n","            LS_fs[trial, mIndex] = -trueLoss                              # Store the \"true\" negative mean-squared error\n","            print(f\"[(worker {worker_id}/{nWorkers}) LeastSq   trial {trial+1}/{numTrials}, m {m}] LS fHat over test data: {trueLoss:.10f}\")\n","        print()\n","\n","    np.savez(outputFile, \n","            ms=ms, \n","            seldonian_solutions_found=seldonian_solutions_found,\n","            seldonian_fs=seldonian_fs, \n","            seldonian_failures_g1=seldonian_failures_g1, \n","            seldonian_failures_g2=seldonian_failures_g2,\n","            seldonian_failures_g3=seldonian_failures_g3,\n","            LS_solutions_found=LS_solutions_found,\n","            LS_fs=LS_fs,\n","            LS_failures_g1=LS_failures_g1,\n","            LS_failures_g2=LS_failures_g2,\n","            LS_failures_g3=LS_failures_g3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_WJD9By572fh"},"source":["## Simple comparison with other algoritms"]},{"cell_type":"markdown","metadata":{"id":"xGHFiTHPGCUD"},"source":["### Simulation"]},{"cell_type":"code","metadata":{"id":"HafVNF0O713D"},"source":["np.random.seed(1234)  # Create the random number generator to use, with seed zero\n","numPoints = 5000   # Let's use 5000 points\n","\n","(X,Y)  = generateData(numPoints)  # Generate the data\n","\n","train_x, test_x, train_y, test_y = train_test_split(X, Y, random_state=1234)\n","\n","# Create the behavioral constraints - each is a gHat function and a confidence level delta\n","gHats  = [g_eq_odds_diff(0.1, batch_size=300)] # The 1st gHat requires MSE < 2.0. The 2nd gHat requires MSE > 1.25\n","deltas = [0.1]\n","\n","(result, found) = QSA(train_x, train_y, gHats, deltas) # Run the Quasi-Seldonian algorithm\n","if found:\n","  print(\"Params with constraints\", result)\n","  print(\"fHat of solution (computed over all data, D):\", fHat(result, train_x, train_y))\n","else:\n","  print(\"No solution found\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-kv12JRD8GYA"},"source":["def seldonian_predict_factory(theta):\n","  def _(x):\n","    return predict(theta, x)\n","  return _\n","\n","print(calc_fp_diff(test_x, test_y, predict_prob=seldonian_predict_factory(result)))\n","print(calc_demographic_parity_diff(test_x, test_y, predict_prob=seldonian_predict_factory(result)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"12yE_SQkiQ49"},"source":["from sklearn.metrics import classification_report\n","\n","y_pred = []\n","for x in test_x:\n","  y_pred.append(round(predict(result, [1, *x])))\n","\n","print(classification_report(test_y, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUNH9rgF8LCO"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","# # Number of trees in random forest\n","# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n","# # Number of features to consider at every split\n","# max_features = ['auto', 'sqrt']\n","# # Maximum number of levels in tree\n","# max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n","# max_depth.append(None)\n","# # Minimum number of samples required to split a node\n","# min_samples_split = [2, 5, 10]\n","# # Minimum number of samples required at each leaf node\n","# min_samples_leaf = [1, 2, 4]\n","# # Method of selecting samples for training each tree\n","# bootstrap = [True, False]\n","# # Create the random grid\n","# random_grid = {'n_estimators': n_estimators,\n","#                'max_features': max_features,\n","#                'max_depth': max_depth,\n","#                'min_samples_split': min_samples_split,\n","#                'min_samples_leaf': min_samples_leaf,\n","#                'bootstrap': bootstrap}\n","\n","# rf = RandomForestClassifier()\n","# clf = RandomizedSearchCV(estimator = rf, \n","#                                param_distributions = random_grid, \n","#                                n_iter = 100, \n","#                                cv = 3, \n","#                                verbose=2, \n","#                                random_state=42, \n","#                                n_jobs = -1)\n","\n","clf = LogisticRegressionCV(cv=5, random_state=0)\n","\n","clf.fit(train_x, train_y)\n","\n","male_indices = test_x[:, 0] == 1\n","female_indices = test_x[:, 0] == 0\n","\n","print(\n","    np.abs(\n","      calc_fp_sklearn(test_x[male_indices], test_y[male_indices], clf) - \n","      calc_fp_sklearn(test_x[female_indices], test_y[female_indices], clf)\n","    )\n",")\n","print(\n","    np.abs(\n","      calc_demographic_parity_sklearn(test_x[male_indices], test_y[male_indices], clf) - \n","      calc_demographic_parity_sklearn(test_x[female_indices], test_y[female_indices], clf)\n","    )\n",")\n","print(classification_report(test_y, clf.predict(test_x)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6nfUlg7SGI1D"},"source":["### Real data - Kaggle\n","https://www.kaggle.com/spscientist/students-performance-in-exams"]},{"cell_type":"markdown","metadata":{"id":"AKE7DP1061ly"},"source":["#### Data processing"]},{"cell_type":"code","metadata":{"id":"gjgCzeR3GLOn"},"source":["from scipy.stats import zscore\n","\n","df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fair AI/german_credit_data.csv')\n","df['Sex'] = df['Sex'].apply(lambda x: 0 if x == 'female' else 1)\n","\n","df['Age'] = zscore(df['Age'])\n","df['Credit amount'] = zscore(df['Credit amount'])\n","df['Duration'] = zscore(df['Duration'])\n","\n","df['Job'] = pd.factorize(df['Job'])[0]\n","df['Housing'] = pd.factorize(df['Housing'])[0]\n","df['Saving accounts'] = pd.factorize(df['Saving accounts'])[0]\n","df['Checking account'] = pd.factorize(df['Checking account'])[0]\n","df['Purpose'] = pd.factorize(df['Purpose'])[0]\n","\n","# df['Math_PassStatus'] = np.where(df['math score'] < 60, 0, 1)\n","# df['gender'] = df['gender'].apply(lambda x: 0 if x == 'female' else 1)\n","# df['race/ethnicity'] = pd.factorize(df['race/ethnicity'])[0]\n","# df['parental level of education'] = pd.factorize(df['parental level of education'])[0]\n","# df['lunch'] = pd.factorize(df['lunch'])[0]\n","# df['test preparation course'] = pd.factorize(df['test preparation course'])[0]\n","# df['reading score'] = zscore(df['reading score'])\n","# df['writing score'] = zscore(df['writing score'])\n","df = df.dropna()\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3rc77h0C63Xy"},"source":["#### Modeling"]},{"cell_type":"markdown","metadata":{"id":"RHXHllZya9Yg"},"source":["##### Seldonian"]},{"cell_type":"code","metadata":{"id":"y8HqPRqNGYjg"},"source":["# Create the behavioral constraints - each is a gHat function and a confidence level delta\n","gHats  = [g_fp_diff(0.025, batch_size=100)]\n","deltas = [0.5]\n","\n","# X = df[['gender', 'race/ethnicity', 'parental level of education', 'lunch', 'test preparation course', 'reading score', 'writing score']].values\n","X = df[['Sex', 'Job', 'Housing', 'Saving accounts', 'Checking account', 'Age', 'Credit amount', 'Duration', 'Purpose']].values\n","Y = df['approval'].values\n","\n","train_x, test_x, train_y, test_y = train_test_split(X, Y, random_state=1234)\n","\n","(result, found) = QSA(\n","    train_x, \n","    train_y, \n","    gHats, \n","    deltas\n",") # Run the Quasi-Seldonian algorithm\n","if found:\n","  print(\"Params with constraints\", result)\n","  print(\"fHat of solution (computed over all data, D):\", fHat(result, X, Y))\n","else:\n","  print(\"No solution found\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uDgs9DXMJJmq"},"source":["def seldonian_predict_factory(theta):\n","  def _(x):\n","    return predict(x, theta)\n","  return _\n","\n","print(calc_fp_diff(test_x, test_y, predict_prob=seldonian_predict_factory(result)))\n","print(calc_demographic_parity_diff(test_x, test_y, predict_prob=seldonian_predict_factory(result)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UhxC9jxilacv"},"source":["from sklearn.metrics import classification_report\n","\n","y_pred = []\n","for x in test_x:\n","  y_pred.append(round(predict(result, [1, *x])))\n","\n","print(classification_report(test_y, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dLDjwNPObJDF"},"source":["##### FairLearn"]},{"cell_type":"markdown","metadata":{"id":"vG60otC3bGQM"},"source":["##### Raw model"]},{"cell_type":"code","metadata":{"id":"gdDpMhF4JOfO"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.metrics import classification_report\n","\n","clf = LogisticRegressionCV(cv=5, random_state=0)\n","clf.fit(train_x, train_y)\n","\n","# def rand_forest_predict_prob(x):\n","#   probs = clf.predict_proba(np.array(x)[1:].reshape((1, -1)))\n","#   return np.argmax(x)\n","\n","# calc_fp_diff(X, Y, predict_prob=rand_forest_predict_prob)\n","male_indices = test_x[:, 0] == 1\n","female_indices = test_x[:, 0] == 0\n","print(\n","    np.abs(\n","      calc_fp_sklearn(test_x[male_indices], test_y[male_indices], clf) - \n","      calc_fp_sklearn(test_x[female_indices], test_y[female_indices], clf)\n","    )\n",")\n","print(\n","    np.abs(\n","      calc_demographic_parity_sklearn(test_x[male_indices], test_y[male_indices], clf) - \n","      calc_demographic_parity_sklearn(test_x[female_indices], test_y[female_indices], clf)\n","    )\n",")\n","print(classification_report(test_y, clf.predict(test_x)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8VZwGlu66uSE"},"source":["### Real data - Algebra Nation"]},{"cell_type":"markdown","metadata":{"id":"hkUhmwRW6yiI"},"source":["#### Data processing"]},{"cell_type":"code","metadata":{"id":"bQ3EarxZ6xV7"},"source":["import pandas as pd\n","\n","df_students = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fair AI/algebra_nation/students.csv')\n","df_logs = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fair AI/algebra_nation/logs.csv')\n","df_sessions = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fair AI/algebra_nation/sessions.csv')\n","df_assessments = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fair AI/algebra_nation/assessments.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8lZ9725TfbAk"},"source":["df_students[df_students['grade'] == 12].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GR7Ie7hl7h9e"},"source":["df_students_filtered = df_students[~pd.isna(df_students['gender'])]\n","df_students_filtered.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmHaWRuF_GCm"},"source":["##### Map video id to logs"]},{"cell_type":"code","metadata":{"id":"0MrK7NX19UV7"},"source":["log_video_id_map = dict()\n","\n","for idx, row in df_logs[['id', 'field_name', 'field_val']].iterrows():\n","  field_name = row.get('field_name')\n","  log_id = row.get('id')\n","  if field_name == 'video_id':\n","    log_video_id_map[log_id] = row.get('field_val')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-NcyyFL_KkF"},"source":["df_logs['video_id'] = df_logs['id'].apply(lambda x: log_video_id_map.get(x, None))\n","df_logs.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WSXKxr3U_h-T"},"source":["##### Map user id to logs"]},{"cell_type":"code","metadata":{"id":"1qytxOOZ_09c"},"source":["df_logs_merged = df_logs.set_index('session_log_id').join(df_sessions[['id', 'useraccount_id', 'school_id', 'subject_id', 'sub_subject_id', 'os']].set_index('id'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KXZuMpOAnMj"},"source":["df_logs_merged[df_logs_merged.useraccount_id.isin(df_students[df_students['grade'] == 12]['useraccount_id'].unique())].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SeHiv0K2Edr4"},"source":["##### Aggregate assessments"]},{"cell_type":"code","metadata":{"id":"5V9Uuh1bEpJA"},"source":["df_assessments.head(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I58eeLr-BW0J"},"source":["df_assessments_agg = df_assessments.groupby('id')['user_gave_correct_answer'].sum().reset_index().rename(columns={'user_gave_correct_answer': 'num_correct_answer'})\n","df_assessments_agg = df_assessments_agg.set_index('id')\\\n","  .join(df_assessments[['id', 'useraccount_id', 'session_log_id', 'subject_id', 'section_folder_id', 'number_of_questions', 'is_finished', 'is_adaptive', 'test_type', 'ts_created']]\\\n","  .set_index('id'))\\\n","  .reset_index()\\\n","  .drop_duplicates(['id'])\n","\n","df_assessments_agg = df_assessments_agg.set_index('id').join(df_assessments.groupby('id')['time_taken'].sum().reset_index().set_index('id')).reset_index()\n","\n","# merge with sessions\n","df_sessions_tmp = df_sessions[['id', 'school_id', 'browser', 'os']].set_index('id')\n","df_assessments_agg = df_assessments_agg.set_index('session_log_id').join(df_sessions_tmp).reset_index().rename(columns={'index': 'session_log_id'})\n","\n","# merge with students\n","df_students_tmp = df_students_filtered[['useraccount_id', 'grade', 'hispanic_ethnicity', 'race', 'gender']].set_index('useraccount_id')\n","df_assessments_agg = df_assessments_agg.set_index('useraccount_id').join(df_students_tmp).reset_index()\n","\n","df_assessments_agg.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TIbtYHPFSmHb"},"source":["##### merge logs to assessments"]},{"cell_type":"code","metadata":{"id":"lTlI3KXRIFED"},"source":["df_assessments_agg_filtered = df_assessments_agg.dropna(subset=['subject_id', 'section_folder_id', 'time_taken', 'school_id', 'grade', 'browser', 'os', 'race'])\n","df_assessments_agg_filtered.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l00CTSbBT1XS"},"source":["df_logs_merged['ts_created_dt'] = pd.to_datetime(df_logs_merged['ts_created'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xDLoErOzRl05"},"source":["import numpy as np\n","from tqdm.auto import tqdm\n","\n","res = []\n","memo = {}\n","\n","actions = df_logs_merged['action_name'].dropna().unique()\n","\n","for user, createdAt, assess_id in zip(tqdm(df_assessments_agg_filtered['useraccount_id']), df_assessments_agg_filtered['ts_created'], df_assessments_agg_filtered['id']):\n","  data = df_logs_merged[df_logs_merged['useraccount_id'].isin([user])]\n","  data = data[(data['ts_created_dt'] <= pd.to_datetime(createdAt))].sort_values('ts_created_dt')\n","  \n","  data['duration'] = [*pd.Series(np.diff(data['ts_created_dt'])).apply(lambda x: x.seconds).values, 0]\n","  data['duration'] = data['duration'].apply(lambda x: x if x <= 60 * 100 else 0)\n","  frequency = data.groupby(['useraccount_id', 'action_name'])['id'].count().reset_index().rename(columns={'id': 'count'}).set_index('action_name')\n","  duration = data.groupby(['useraccount_id', 'action_name'])['duration'].sum().reset_index().set_index('action_name')\n","\n","  assessments = df_assessments_agg_filtered[\n","    (df_assessments_agg_filtered['useraccount_id'] == user) &\n","    (df_assessments_agg_filtered['ts_created'] < createdAt)\n","  ]\n","\n","  user_res = dict(assess_id=assess_id)\n","  for action in actions:\n","    try:\n","      user_res[f'{action} frequency'] = frequency.loc[action].get('count', 0)\n","    except:\n","      user_res[f'{action} frequency'] = 0\n","\n","    try:\n","      user_res[f'{action} duration'] = duration.loc[action].get('duration', 0)\n","    except:\n","      user_res[f'{action} duration'] = 0\n","\n","  if assessments.shape[0] > 0:\n","    user_res['history_avg_num_correct_answer'] = assessments['num_correct_answer'].mean()\n","    user_res['history_total_num_correct_answer'] = assessments['num_correct_answer'].sum()\n","    user_res['assessment_order'] = assessments.shape[0] + 1\n","    user_res['prev_num_correct_answer'] = assessments.tail(1)['num_correct_answer'].values[0]\n","  else:\n","    user_res['history_total_num_correct_answer'] = 0\n","    user_res['history_avg_num_correct_answer'] = 0\n","    user_res['assessment_order'] = 1\n","    user_res['prev_num_correct_answer'] = -1\n","\n","  res.append(user_res)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iwGv6mVQFsMu"},"source":["df_assessments_agg_filtered = df_assessments_agg_filtered.set_index('id').join(pd.DataFrame(res).set_index('assess_id')).reset_index()\n","df_assessments_agg_filtered.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQRDvYasXLj7"},"source":["df_assessments_agg_filtered.to_csv('/content/drive/My Drive/Colab Notebooks/Fair AI/algebra_nation/aggregated_assessments_v5.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9RluzVFB67B3"},"source":["#### Modeling"]},{"cell_type":"code","metadata":{"id":"9cXJ449FZAtQ"},"source":["df_assessments_agg_filtered = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fair AI/algebra_nation/aggregated_assessments_v5.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUDU2l-fDWg6"},"source":["sample = df_assessments_agg_filtered[df_assessments_agg_filtered['useraccount_id'].isin(pd.Series(df_assessments_agg_filtered['useraccount_id'].unique()).sample(30))]\n","sample['class_id'] = sample['school_id'].apply(lambda x: 1 if x == 21407.0 else 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4hTCDhgFZH1"},"source":["sample.to_json('./assessments.json', orient='records')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P_FiUKA8q4w1"},"source":["!pip install -U plotly kaleido"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qn4zcf0peMBp"},"source":["subset = df_assessments_agg_filtered[df_assessments_agg_filtered['grade'].isin([12.0])]\n","subset['race'] = subset['race'].apply(lambda x: 'White' if x == 'Caucasian' else 'Non-white')\n","subset['gender'] = subset['gender'].apply(lambda x: 'Female' if x == 'f' else 'Male')\n","subset['pass_status'] = (subset['num_correct_answer'] / subset['number_of_questions']) >= 0.6\n","subset['pass_status'] = subset['pass_status'].apply(lambda x: 'Passed' if x else 'Failed')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XOHMcbqGpA44"},"source":["subset['race'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A3ITYUGZsNih"},"source":["subset['gender'].value_counts().reset_index()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YALfhzt1jXok"},"source":["import plotly.express as px"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"emVkf4oNqRLd"},"source":["import plotly.express as px\n","fig = px.bar(\n","    subset['race'].replace('Black or African American', 'Blk/African American').replace('Two or More Races', 'Multi-races')\\\n","    .value_counts().reset_index().rename(columns={\"index\":\"race\", 'race': \"number of assessments\"}), \n","    x='race', \n","    y='number of assessments',\n","    color='race',\n","    color_discrete_sequence=['#577590', '#f9c74f', '#f9c74f', '#f9c74f', '#f9c74f'],\n",")\n","\n","fig.update_layout(\n","    showlegend=False,\n","    font_size=22\n",")\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVxk-FDUi_bi"},"source":["fig.write_image(\"race.png\", width=1024, height=768, scale=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQy9uBtwsiC3"},"source":["import plotly.express as px\n","fig = px.bar(\n","    subset['gender'].value_counts().reset_index().replace('f', 'Female').replace('m', 'Male').rename(columns={\"index\":\"gender\", 'gender': \"number of assessments\"}), \n","    x='gender', \n","    y='number of assessments',\n","    color='gender',\n","    color_discrete_sequence=['#ef476f', '#118ab2']\n",")\n","\n","fig.update_layout(\n","    showlegend=False,\n","    font_size=22\n",")\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7QiWcSDqxco"},"source":["fig.write_image(\"gender.png\", width=1024, height=768, scale=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CNC_7t-5oByZ"},"source":["df_gender_pass = subset.groupby(['gender', 'pass_status'])['id'].count().reset_index().rename(columns={'id': 'count'})\n","fig = px.bar(\n","    df_gender_pass, \n","    x=\"gender\", \n","    y=\"count\", \n","    color=\"pass_status\", \n","    barmode=\"group\", \n","    color_discrete_sequence=['#ef476f', '#118ab2']\n",")\n","\n","fig.update_layout(\n","    font_size=22\n",")\n","\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRpuAaKEaj19"},"source":["fig.write_image(\"gender_pass_status.png\", width=1024, height=768, scale=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiObvTtLoGmh"},"source":["df_gender_pass = subset.groupby(['race', 'pass_status'])['id'].count().reset_index().rename(columns={'id': 'count'})\n","fig = px.bar(\n","    df_gender_pass, \n","    x=\"race\",\n","    y=\"count\", \n","    color=\"pass_status\", \n","    barmode=\"group\", \n","    color_discrete_sequence=['#ef476f', '#118ab2']\n","  )\n","fig.update_layout(\n","    font_size=22\n",")\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1pQFZ5ibd7_"},"source":["fig.write_image(\"race_pass_status.png\", width=1024, height=768, scale=4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b7cW1wS3XV1n"},"source":["from sklearn import preprocessing\n","\n","df_assessments_agg_filtered = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fair AI/algebra_nation/aggregated_assessments_v5.csv')\n","df_logs = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fair AI/algebra_nation/logs.csv')\n","\n","# filtering\n","df_assessments_agg_filtered = df_assessments_agg_filtered[df_assessments_agg_filtered['grade'].isin([12.0])]\n","df_assessments_agg_filtered = df_assessments_agg_filtered[df_assessments_agg_filtered['is_finished'] == 1]\n","\n","# available_actions = df_logs['action_name'].dropna().unique()\n","available_actions = [\n","  'video_watch', 'video_pause', 'video_play', \n","  'video_seek', 'video_completed', \n","  'tys_review_correct_question', 'tys_review_solution_video', 'tys_review_topic_video',\n","  # 'wall_make_post', 'wall_search'\n","]\n","\n","df_assessments_agg_filtered['pass_status'] = \\\n","  (df_assessments_agg_filtered['num_correct_answer'] / df_assessments_agg_filtered['number_of_questions']) >= 0.6\n","\n","numeric_cols = []\n","numeric_cols = [f'{action} frequency' for action in available_actions]\n","# numeric_cols = [*numeric_cols, *[f'{action} duration' for action in available_actions]]\n","numeric_cols = ['history_avg_num_correct_answer', 'assessment_order', *numeric_cols]\n","\n","factor_cols = [\n","  'race',\n","  'gender',\n","  # 'grade',\n","  'subject_id', \n","  'school_id', \n","  # 'browser', 'os',\n","]\n","wanted_cols = [\n","  *factor_cols,\n","  *numeric_cols\n","]\n","\n","for col in factor_cols:\n","  if col == 'gender':\n","    df_assessments_agg_filtered[col] = df_assessments_agg_filtered[col].apply(lambda x: 1 if x == 'm' else 0)\n","  if col == 'race':\n","    df_assessments_agg_filtered[col] = df_assessments_agg_filtered[col].apply(lambda x: 1 if x == 'Black or African American' else 0)\n","  else:\n","    df_assessments_agg_filtered[col] = pd.factorize(df_assessments_agg_filtered[col])[0]\n","\n","df_assessments_agg_filtered[numeric_cols] = preprocessing.scale(df_assessments_agg_filtered[numeric_cols])\n","\n","df_assessments_agg_filtered['pass_status_factor'] = df_assessments_agg_filtered['pass_status'].apply(lambda x: 1 if x else 0)\n","\n","df_assessments_agg_filtered[[*wanted_cols, 'pass_status_factor']].head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hH2A71N-ZL-z"},"source":["from sklearn.linear_model import LinearRegression\n","from sklearn.svm import SVC\n","from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n","\n","grade_predictor = SVC(gamma='auto')\n","\n","X = df_assessments_agg_filtered[wanted_cols].values\n","Y = df_assessments_agg_filtered['num_correct_answer'].values\n","\n","train_x, test_x, train_y, test_y = train_test_split(X, Y, random_state=1234)\n","\n","grade_predictor.fit(train_x, train_y)\n","print(grade_predictor.score(train_x, train_y))\n","\n","df_assessments_agg_filtered['predicted_num'] = grade_predictor.predict(X)\n","df_assessments_agg_filtered['predicted_num'] = preprocessing.scale(df_assessments_agg_filtered['predicted_num'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"In3POR35B-4a"},"source":["##### QSA"]},{"cell_type":"code","metadata":{"id":"cN_nq81KZNfW"},"source":["X = df_assessments_agg_filtered[[*wanted_cols, 'predicted_num']].values\n","Y = df_assessments_agg_filtered['pass_status_factor'].values\n","\n","train_x, test_x, train_y, test_y = train_test_split(X, Y, random_state=1234)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C13m8TveJjFY"},"source":["# gHats  = [g_eq_odds_diff(0.03, batch_size=150, bootstrap_num=300)]\n","gHats  = [g_max_loss(0.15), g_eq_odds_diff(0.001, batch_size=200, bootstrap_num=64)]\n","deltas = [0.1, 0.1]\n","\n","(result, found) = QSA(\n","    train_x,\n","    train_y,\n","    gHats,\n","    deltas\n",") # Run the Quasi-Seldonian algorithm\n","if found:\n","  print(\"Params with constraints\", result)\n","  print(\"fHat of solution (computed over all data, D):\", fHat(result, X, Y))\n","else:\n","  print(\"No solution found\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HsiFe3K41XQb"},"source":["```python\n","# 12th grade\n","\n","####################START OF GENDER#####################\n","\n","# (1)\n","###############\n","gHats  = [g_max_loss(0.15), g_eq_odds_diff(0.01, batch_size=200, bootstrap_num=64)]\n","deltas = [0.1, 0.1]\n","results = [-0.49422,  0.57848, -1.47481, -0.19909, -0.00524,  0.62251,\n","        0.1279 ,  0.29416,  0.25328, -0.32137,  0.20153, -0.24534,\n","       -0.08981, -0.15302,  0.02212,  0.63059]\n","eq_odds = 0.0726\n","###############\n","\n","# (2)\n","###############\n","gHats  = [g_max_loss(0.15), g_eq_odds_diff(0.05, batch_size=200, bootstrap_num=64)]\n","deltas = [0.1, 0.1]\n","\n","results = [\n","  -0.49422,  0.57848, -1.47481, -0.19916, -0.00747,  0.61705,\n","  -0.10081,  0.35104,  0.25394, -0.32137,  0.20153, -0.24534,\n","  -0.17246, -0.03464,  0.02665,  0.61185\n","]\n","\n","# gender\n","eq_odds = 0.08564\n","###############\n","\n","# (3)\n","###############\n","###############\n","\n","####################END OF GENDER#####################\n","\n","\n","####################START OF RACE#####################\n","\n","###############\n","g_eq_odds_diff(0.25)\n","results = [\n","  -1.4812 , -0.07405,  0.16733, -0.01593, -0.00515,  0.6227 ,\n","  -0.024  ,  0.29256,  0.2464 , -0.3248 ,  0.22267, -0.25594,\n","  -0.08992,  0.04587,  0.02941,  0.63061\n","]\n","\n","# race\n","eq_odds = 0.2082\n","###############\n","\n","###############\n","g_eq_odds_diff(0.2)\n","results = [\n","  -1.62982,  0.16682,  0.10559,  0.11313, -0.00512,  0.32857,\n","  -0.06024,  0.28862,  0.2536 , -0.32152,  0.21349, -0.24508,\n","  -0.09937,  0.09117,  0.02944,  0.63061\n","]\n","\n","# race\n","eq_odds = 0.17937\n","###############\n","\n","###############\n","g_eq_odds_diff(0.125)\n","results = [\n","  -1.4812 , -0.08077,  0.10554, -0.00644, -0.00515,  0.61669,\n","  -0.02142,  0.29283,  0.25304, -0.32152,  0.20212, -0.24574,\n","  -0.08625,  0.04564,  0.02938,  0.08865\n","]\n","\n","# race\n","eq_odds = 0.11936\n","###############\n","\n","###############\n","g_eq_odds_diff(0.05)\n","results = [\n","  -2.45165, -0.37855,  1.06118,  0.02282, -0.00513,  0.25044,\n","  0.13393,  0.18211,  0.25874, -0.32149,  0.20146, -0.24537,\n","  -0.08767,  0.06238,  0.02938,  0.63061\n","]\n","\n","# race\n","eq_odds = 0.0673\n","###############\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"Wy4yGRBgaXML"},"source":["def seldonian_predict_factory(theta):\n","  def _(x):\n","    return predict(x, theta)\n","  return _\n","\n","print(calc_eq_odds_diff(test_x, test_y, predict_prob=seldonian_predict_factory(result)))\n","print(calc_eq_op_diff(test_x, test_y, predict_prob=seldonian_predict_factory(result)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BLe5MUf5a3n3"},"source":["race_results = [\n","  [\n","    -1.4812 , -0.07405,  0.16733, -0.01593, -0.00515,  0.6227 ,\n","    -0.024  ,  0.29256,  0.2464 , -0.3248 ,  0.22267, -0.25594,\n","    -0.08992,  0.04587,  0.02941,  0.63061\n","  ],\n","  [\n","    -1.62982,  0.16682,  0.10559,  0.11313, -0.00512,  0.32857,\n","    -0.06024,  0.28862,  0.2536 , -0.32152,  0.21349, -0.24508,\n","    -0.09937,  0.09117,  0.02944,  0.63061\n","  ],\n","  [\n","    -1.4812 , -0.08077,  0.10554, -0.00644, -0.00515,  0.61669,\n","    -0.02142,  0.29283,  0.25304, -0.32152,  0.20212, -0.24574,\n","    -0.08625,  0.04564,  0.02938,  0.08865\n","  ],\n","  [\n","    -2.45165, -0.37855,  1.06118,  0.02282, -0.00513,  0.25044,\n","    0.13393,  0.18211,  0.25874, -0.32149,  0.20146, -0.24537,\n","    -0.08767,  0.06238,  0.02938,  0.63061\n","  ]\n","]\n","\n","gender_results = [\n","  [\n","    -0.49422,  0.57848, -1.47481, -0.19909, -0.00524,  0.62251,\n","    0.1279 ,  0.29416,  0.25328, -0.32137,  0.20153, -0.24534,\n","    -0.08981, -0.15302,  0.02212,  0.63059\n","  ],\n","  [\n","    -0.49422,  0.57848, -1.47481, -0.19916, -0.00747,  0.61705,\n","    -0.10081,  0.35104,  0.25394, -0.32137,  0.20153, -0.24534,\n","    -0.17246, -0.03464,  0.02665,  0.61185\n","  ]\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n3aVTNpvabN8"},"source":["from sklearn.metrics import classification_report\n","\n","def seldonian_predict_factory(theta):\n","  def _(x):\n","    return predict(x, theta)\n","  return _\n","\n","for result in race_results:\n","  y_pred = []\n","  y_pred_prob = []\n","  for x in test_x:\n","    y_pred.append(round(predict(result, [1, *x])))\n","    y_pred_prob.append(predict(result, [1, *x]))\n","\n","  print(calc_eq_odds_diff(test_x, test_y, predict_prob=seldonian_predict_factory(result)))\n","  print(classification_report(test_y, y_pred))\n","  fpr, tpr, thresholds = roc_curve(test_y, y_pred_prob)\n","  print(auc(fpr, tpr))\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LtW5b6Fz9nrX"},"source":["###### Explanation"]},{"cell_type":"code","metadata":{"id":"JeCbBY-N9rpO"},"source":["from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n","import numpy as np\n","\n","# race - 0.25\n","result = [\n","  -1.4812 , -0.07405,  0.16733, -0.01593, -0.00515,  0.6227 ,\n","  -0.024  ,  0.29256,  0.2464 , -0.3248 ,  0.22267, -0.25594,\n","  -0.08992,  0.04587,  0.02941,  0.63061\n","]\n","clf = LogisticRegression(random_state=1234)\n","clf.fit(train_x, train_y)\n","clf.coef_ = np.array([result[1:]])\n","clf.intercept_ = np.array([result[0]])\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# clf = RandomForestClassifier(random_state=1234)\n","# clf = LogisticRegressionCV(cv=5, random_state=0, max_iter=10000)\n","# clf.fit(train_x, train_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sF3-iJ3G94fK"},"source":["import dalex as dx\n","\n","data = pd.DataFrame(train_x, columns=[*wanted_cols, 'predicted_num'])\n","\n","exp = dx.Explainer(clf, data, train_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awPESbWy-H0N"},"source":["stu = data.loc[0,:]\n","\n","bd_stu = exp.predict_parts(stu, type='break_down')\n","bd_interactions_stu = exp.predict_parts(stu, type='break_down_interactions')\n","sh_stu = exp.predict_parts(stu, type='shap', B=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O5JX8hh2-oYV"},"source":["bd_stu.result.label = \"Student 11\"\n","bd_interactions_stu.result.label = \"Student 11+\"\n","\n","bd_stu.plot(bd_interactions_stu, max_vars=30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rr4_M9PYGTOF"},"source":["sh_stu.result.label = \"Student 11\"\n","\n","sh_stu.plot(bar_width = 16, max_vars=30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vRa-wl0DKSna"},"source":["vi = exp.model_parts()\n","\n","vi.result.label = \"Model\"\n","\n","vi.plot(max_vars=30)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9V9aVRkcK4Il"},"source":["pdp_num = exp.model_profile(type = 'partial')\n","pdp_num.result[\"_label_\"] = 'Model'\n","\n","pdp_num.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UoUkaDmJcfZs"},"source":["###### Eq Odds trend"]},{"cell_type":"code","metadata":{"id":"8lndWp5Mcsik"},"source":["race_performance = pd.DataFrame([\n","  {'Required Max Equalized odds': 0.25, 'F1': 0.78},\n","  {'Required Max Equalized odds': 0.20, 'F1': 0.77},\n","  {'Required Max Equalized odds': 0.15, 'F1': 0.72},\n","  {'Required Max Equalized odds': 0.10, 'F1': 0.69},\n","])\n","\n","import plotly.express as px\n","import plotly.graph_objects as go\n","\n","fig = go.Figure()\n","fig.add_trace(\n","    go.Scatter(x=race_performance['Required Max Equalized odds'], y=race_performance['F1'],\n","                    mode='lines+markers',\n","                    name='lines+markers')\n",")\n","# fig = px.scatter(race_performance, x=\"Equalized odds\", y=\"F1\", title='Fairness & Performance Trade-off (Race)')\n","fig.update_layout(\n","    xaxis_title=\"Required Max Equalized Odds\",\n","    yaxis_title=\"F-measure\",\n","    font=dict(\n","        size=16,\n","        color=\"#7f7f7f\"\n","    )\n",")\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_DKqVr4nhbWX"},"source":["fig.write_image(\"perf_fair_tradeoff.png\", scale=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mt1ilvKySaIE"},"source":["###### Model fairness comparison"]},{"cell_type":"code","metadata":{"id":"A_PcbMDwRM5t"},"source":["import pandas as pd\n","\n","df_performance = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Fair AI/model_performance.csv')\n","df_performance['Model'] = df_performance['Model'].str.replace('SA_odds_', 'Fair-LR@')\n","\n","# adjust the order of eo@0.01 and eo@0.05\n","temp = df_performance.iloc[11].copy()\n","df_performance.iloc[11] = df_performance.iloc[10]\n","df_performance.iloc[10] = temp\n","\n","df_performance"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFrqXJOFSkAG"},"source":["import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","colors = {\n","  \"race\": ['#7CAFC4', '#7CAFC4', '#7CAFC4', '#5995ED', '#5995ED', '#5995ED', '#5995ED'],\n","  \"gender\": ['#7CAFC4', '#7CAFC4', '#7CAFC4', '#5995ED', '#5995ED']\n","}\n","fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Model Fairness - Race\", \"Model Fairness - Gender\"))\n","\n","for idx, group in enumerate(df_performance['Group'].unique()):\n","  data = df_performance[df_performance['Group'] == group]\n","  fig.add_trace(\n","    go.Bar(\n","    x=data['Model'],\n","    y=data['Equalized Odds'],\n","    marker_color=colors[group] # marker color can be a single color value or an iterable\n","  ), row=1, col=idx+1)\n","\n","fig.update_layout(\n","    showlegend=False,\n","    font_size=16\n",")\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HY1B0Et0XbB0"},"source":["fig.write_image(\"model_comparison.png\", width=1400, height=768, scale=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BTvdxXmrJyh3"},"source":["###### Decision boundary"]},{"cell_type":"markdown","metadata":{"id":"PvAmx13QaKLQ"},"source":["https://mc.ai/decision-boundary-plotting-for-high-dimensional-data/"]},{"cell_type":"code","metadata":{"id":"DhkcQT8OIfN1"},"source":["from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n","\n","# race\n","# result = [\n","#   -1.4812 , -0.07405,  0.16733, -0.01593, -0.00515,  0.6227 ,\n","#   -0.024  ,  0.29256,  0.2464 , -0.3248 ,  0.22267, -0.25594,\n","#   -0.08992,  0.04587,  0.02941,  0.63061\n","# ]\n","\n","# gender\n","# result = [\n","#   -0.49422,  0.57848, -1.47481, -0.19909, -0.00524,  0.62251,\n","#   0.1279 ,  0.29416,  0.25328, -0.32137,  0.20153, -0.24534,\n","#   -0.08981, -0.15302,  0.02212,  0.63059\n","# ]\n","# clf = LogisticRegression(random_state=1234)\n","# clf.fit(train_x, train_y)\n","# clf.coef_ = np.array([result[1:]])\n","# clf.intercept_ = np.array([result[0]])\n","\n","clf = LogisticRegressionCV(cv=5, random_state=0, max_iter=10000)\n","clf.fit(train_x, train_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7X9Ej-FBKVxJ"},"source":["from sklearn.manifold import TSNE# set random seed\n","seed = 1\n","np.random.seed(seed) # transform original data into 2D\n","\n","rand_indice = np.random.randint(train_x.shape[0], size=500)\n","# plot_x = pd.DataFrame(train_x[rand_indice, :]).append(pd.DataFrame(test_x)).values\n","# plot_y = pd.DataFrame(train_y[rand_indice]).append(pd.DataFrame(test_y)).values\n","\n","plot_x = test_x.copy()\n","plot_y = test_y.copy()\n","\n","race = plot_x[:, 0]\n","tsne = TSNE(n_components=2)\n","X_2d = tsne.fit_transform(plot_x) # shape: (num_samples, 2)# split original and transformed data using same random seed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOgkrtfLYDOr"},"source":["from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier# build and train original model\n","\n","X_train, X_test, y_train, y_test = train_test_split(plot_x, plot_y, train_size=0.99, random_state=seed)\n","X_train_2d, X_test_2d, y_train, y_test = train_test_split(X_2d, plot_y, train_size=0.99, random_state=seed)\n","\n","y_pred = clf.predict(X_train)\n","\n","voronoi = KNeighborsClassifier(n_neighbors=1)\n","voronoi.fit(X_train_2d, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kD5g621AYrOn"},"source":["import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n","from matplotlib.ticker import NullFormatter\n","\n","# 1. generate 2D background grid\n","pad = 0.5\n","min_x1, max_x1 = np.min(X_train_2d[:, 0]) - pad, np.max(X_train_2d[:, 0]) + pad\n","min_x2, max_x2 = np.min(X_train_2d[:, 1]) - pad, np.max(X_train_2d[:, 1]) + pad\n","\n","def generate_grid_points(min_x, max_x, min_y, max_y, resolution=0.1):\n","    \"\"\"Generate resolution * resolution points within a given range.\"\"\"\n","    return np.meshgrid(\n","        np.arange(min_x, max_x, resolution),\n","        np.arange(min_y, max_y, resolution)\n","    )\n","    \n","xx, yy = generate_grid_points(min_x1, max_x1, min_x2, max_x2)\n","\n","# 2. get model's predictions for grid\n","background = voronoi.predict(np.c_[xx.ravel(), yy.ravel()])\n","background = background.reshape(xx.shape)\n","\n","# plot data along with decision boundary\n","fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n","colors = ['#FF0064', '#0839FF']\n","cmap = ListedColormap(['#FF0064', '#0839FF'])\n","\n","# 3. plot grid with predictions (this forms the decision boundary)\n","ax.contourf(xx, yy, background, alpha=0.4, cmap=cmap)\n","\n","protected_attribute = 'gender'\n","col_idx = 0 if protected_attribute == 'gender' else 1\n","# 4. plot training and test data\n","for idx, sensitive in enumerate(X_train[:, col_idx]):\n","  ax.scatter(\n","      X_train_2d[idx, 0], \n","      X_train_2d[idx, 1], \n","      c=colors[y_train[idx]],\n","      marker=\"P\" if sensitive else '.', \n","      s=200,\n","      zorder=10,\n","      edgecolor='#f8f8f8', \n","      linewidth=0.75, \n","  )\n","# ax.scatter(X_test_2d[:, 0], X_test_2d[:, 1], c=y_test,\n","#            cmap=cmap, marker=\"x\", label=\"Test\")\n","\n","# other specifications\n","# ax.set_title(\"Logistic Regression\")\n","ax.set_xlim([min_x1, max_x1])\n","ax.set_ylim([min_x2, max_x2])\n","\n","# https://stackoverflow.com/a/18861258\n","ax.xaxis.set_major_formatter(NullFormatter())\n","ax.yaxis.set_major_formatter(NullFormatter())\n","ax.xaxis.set_ticks_position('none')\n","ax.yaxis.set_ticks_position('none')\n","\n","# Don't allow the axis to be on top of your data\n","# ax.set_axisbelow(True)\n","ax.minorticks_on()\n","# Customize the major grid\n","ax.grid(which='major', linestyle='-', linewidth='1', color='#f8f8f8')\n","# Customize the minor grid\n","ax.grid(which='minor', linestyle='-', linewidth='1', color='#f8f8f8')\n","\n","# ax.legend(loc=\"best\", title=\"Data\")\n","\n","%config InlineBackend.figure_format = 'retina'\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_dUnBPK0Yty"},"source":["!pip install scikit-image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTDxDrrI0GyA"},"source":["from skimage.measure import compare_ssim\n","import cv2\n","import numpy as np\n","\n","before = cv2.imread('./nonfair-gender.png')\n","after = cv2.imread('./fair-gender.png')\n","\n","# Convert images to grayscale\n","before_gray = cv2.cvtColor(before, cv2.COLOR_BGR2GRAY)\n","after_gray = cv2.cvtColor(after, cv2.COLOR_BGR2GRAY)\n","\n","# Compute SSIM between two images\n","(score, diff) = compare_ssim(before_gray, after_gray, full=True)\n","print(\"Image similarity\", score)\n","\n","# The diff image contains the actual image differences between the two images\n","# and is represented as a floating point data type in the range [0,1] \n","# so we must convert the array to 8-bit unsigned integers in the range\n","# [0,255] before we can use it with OpenCV\n","diff = (diff * 255).astype(\"uint8\")\n","\n","# Threshold the difference image, followed by finding contours to\n","# obtain the regions of the two input images that differ\n","thresh = cv2.threshold(diff, 0, 255, cv2.THRESH_BINARY_INV | cv2.THRESH_OTSU)[1]\n","contours = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","contours = contours[0] if len(contours) == 2 else contours[1]\n","\n","mask = np.zeros(before.shape, dtype='uint8')\n","filled_after = after.copy()\n","\n","for c in contours:\n","    area = cv2.contourArea(c)\n","    if area > 40:\n","        x,y,w,h = cv2.boundingRect(c)\n","        cv2.rectangle(before, (x, y), (x + w, y + h), (36,255,12), 2)\n","        cv2.rectangle(after, (x, y), (x + w, y + h), (36,255,12), 2)\n","        cv2.drawContours(mask, [c], 0, (0,255,0), -1)\n","        cv2.drawContours(filled_after, [c], 0, (0,255,0), -1)\n","\n","# cv2.imshow('before', before)\n","# cv2.imshow('after', after)\n","# cv2.imshow('diff',diff)\n","# cv2.imshow('mask',mask)\n","# cv2.imshow('filled after',filled_after)\n","cv2.waitKey(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m29OPdnU2PC5"},"source":["cv2.imwrite('./gender_compared.png', filled_after)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tpy7nwf7CEhL"},"source":["##### ML"]},{"cell_type":"code","metadata":{"id":"IzOPoc_garlX"},"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc\n","from sklearn.svm import SVC\n","\n","# clf = LogisticRegressionCV(cv=5, random_state=0, max_iter=10000)\n","# clf = RandomForestClassifier(random_state=1234)\n","clf = SVC(gamma='auto', probability=True, random_state=1234)\n","clf.fit(train_x, train_y)\n","\n","male_indices = test_x[:, 0] == 1\n","female_indices = test_x[:, 0] == 0\n","\n","y_pred = clf.predict(test_x)\n","print(\"Equalized odds:\", calc_odds_sklearn(test_x, test_y, clf))\n","print(\"Equal Opportunity:\", calc_eq_op_sklearn(test_x, test_y, clf))\n","print(classification_report(test_y, clf.predict(test_x)))\n","\n","fpr, tpr, thresholds = roc_curve(test_y, clf.predict_proba(test_x)[:, 1])\n","print(auc(fpr, tpr))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EvtwB9oPAG7J"},"source":["2*((0.77*0.71)/(0.77+0.71))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ul-Q8bhqCHEn"},"source":["##### Fair-learn"]},{"cell_type":"code","metadata":{"id":"PtHHJSamCL8k"},"source":["from fairlearn.widget import FairlearnDashboard\n","from sklearn.model_selection import train_test_split\n","from fairlearn.reductions import GridSearch\n","from fairlearn.reductions import DemographicParity, ErrorRate, EqualizedOdds\n","\n","sweep = GridSearch(\n","    LogisticRegressionCV(cv=5, random_state=0, max_iter=10000),\n","    constraints=EqualizedOdds(),\n","    grid_size=71\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AaUHW3NUCr87"},"source":["non_sensitive_cols = [col for col in wanted_cols if col != 'race']\n","X = df_assessments_agg_filtered[[*non_sensitive_cols, 'predicted_num']].values\n","Y = df_assessments_agg_filtered['pass_status_factor'].values\n","A = df_assessments_agg_filtered.race.values\n","\n","X_train, X_test, Y_train, Y_test, A_train, A_test = train_test_split(\n","    X,\n","    Y,\n","    A,\n","    random_state=1234,\n","    stratify=Y\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3v4B-PCDQZB"},"source":["sweep.fit(X_train, Y_train, sensitive_features = A_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_dcHSMvxEain"},"source":["y_pred = sweep.predict(X_test)\n","\n","print(classification_report(Y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9HEGvByGZPu"},"source":["from fairlearn.metrics import equalized_odds_difference\n","\n","equalized_odds_difference(Y_test, y_pred, sensitive_features=A_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MoPSTs6vD74p"},"source":["errors, eq_odds = [], []\n","for m in sweep._predictors:\n","    def classifier(X): return m.predict(X)\n","\n","    error = ErrorRate()\n","    error.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train)\n","\n","    odds = EqualizedOdds()\n","    odds.load_data(X_train, pd.Series(Y_train), sensitive_features=A_train)\n","    \n","    errors.append(error.gamma(classifier)[0])\n","    eq_odds.append(odds.gamma(classifier).max())\n","\n","all_results = pd.DataFrame({\"predictor\": sweep._predictors, \"equalized_odds\": eq_odds, 'error': errors})\n","\n","non_dominated = []\n","for row in all_results.itertuples():\n","    errors_for_lower_or_eq_disparity = all_results[\"error\"][all_results[\"equalized_odds\"] <= row.equalized_odds]\n","    if row.error <= errors_for_lower_or_eq_disparity.min():\n","        non_dominated.append(row.predictor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6q6u9ahTMHfZ"},"source":["dashboard_predicted = {}\n","\n","for i in range(len(non_dominated)):\n","    key = \"dominant_model_{0}\".format(i)\n","    value = non_dominated[i].predict(X_test)\n","    dashboard_predicted[key] = value\n","\n","\n","FairlearnDashboard(sensitive_features=A_test, sensitive_feature_names=['race'],\n","                   y_true=Y_test,\n","                   y_pred=dashboard_predicted)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U1TgmRwdiO8i"},"source":["## Run experiment"]},{"cell_type":"code","metadata":{"id":"dOXj2Mzo7xvy"},"source":["ray.shutdown()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwD6vgtjxdAZ","jupyter":{"outputs_hidden":true}},"source":["ray.shutdown()\n","ray.init()\n","\n","# Create the behavioral constraints - each is a gHat function and a confidence level delta\n","gHats  = [gHat1, gHat2, gHat3] # The 1st gHat requires MSE < 2.0. The 2nd gHat requires MSE > 1.25\n","deltas = [0.1, 0.1, 0.1]\n","nWorkers = 8\n","\n","# We will use different amounts of data, m. The different values of m will be stored in ms.\n","# These values correspond to the horizontal axis locations in all three plots we will make.\n","# We will use a logarithmic horizontal axis, so the amounts of data we use shouldn't be evenly spaced.\n","ms   = [2**i for i in range(5, 17)]  # ms = [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]\n","numM = len(ms)\n","\n","# How many trials should we average over?\n","numTrials = 70 # We pick 70 because with 70 trials per worker, and 16 workers, we get >1000 trials for each value of m\n","\n","# How much data should we generate to compute the estimates of the primary objective and behavioral constraint function values \n","# that we call \"ground truth\"? Each candidate solution deemed safe, and identified using limited training data, will be evaluated \n","# over this large number of points to check whether it is really safe, and to compute its \"true\" mean squared error.\n","mTest = ms[-1] * 100 # about 5,000,000 test samples\n","\n","# Start 'nWorkers' threads in parallel, each one running 'numTrials' trials. Each thread saves its results to a file\n","tic = timeit.default_timer()\n","_ = ray.get([run_experiments.remote(worker_id, nWorkers, ms, numM, numTrials, mTest) for worker_id in range(1,nWorkers+1)])\n","toc = timeit.default_timer()\n","time_parallel = toc - tic # Elapsed time in seconds\n","print(f\"Time ellapsed: {time_parallel}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OtvyJXokxh3P"},"source":["(X[:, 1] == 0).sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bl-lZuOcSmgG"},"source":[""],"execution_count":null,"outputs":[]}]}